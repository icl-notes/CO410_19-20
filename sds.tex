\chapter{Scalable Distributed Systems}

\paragraph{
HPC and mainframes.
}
\begin{itemize}
\item Pros: Single solution, powerful hardware, reliable design
\item Cons: £££, failure models (single point of failure), vendor lock-in, adaptability, no incremental scalability
\end{itemize}

\paragraph{Distributed Computing.} Want to achieve linear scalability, but
synchronization, communication (\textbf{overheads}) prevent it. Ideally: perfect
partition of data and compute (e.g. distributed client/server model on \textbf{cheap commodity hardware} (best £/resource); accounts for failures and
manages the network speed bottleneck in software (parallel algorithms and
systems).

\paragraph{Data centers} Usually DC will have non-commodity network, but it will
still be the slowest part in memory hierarchy. Focus on \textbf{scaling-out} not up
by buying cheap hardware in bulk therefore achieve economy of scale. Elasticity:
Resources on-demand (illusion of infinite resource).

\section{Properties of DS}
\begin{itemize}
\item Scalability: By aggregation of many resources
  \begin{itemize}
  \item Compute
  \item Storage: distributed file systems
  \item Memory: cluster memory (in-memory key/value stores, caches)
  \item Bandwidth: DC networks, CDN
  \end{itemize}
  \item Location transparency from the user's perspective (black box API)
  \item High availability: Mask hardware and software failures
  \item Composition of independent services
\end{itemize}
\subsection{Answering a google search request}
\begin{enumerate}
\item Load-balancer routes request to lightly-loaded Google Web Server (GWS)
\item GWS routes search to one Index Server for each shard through load-balancer
\item Results are aggregated (IDs for matching documents), ie ordered by
  relevance
\item GWS sends appropriate IDs to Doc Servers to retrieve URL, title, summary
\item Results are aggregated (+ad, spell), producing search result page
\end{enumerate}

\section{Design principles}
\paragraph{Types of scalable systems}
\begin{enumerate}
\item Online systems ($<$100 ms) (OLTP)
\item Batch processing systems ($>$1 hours) (OLAP)
\item Nearline systems ($<$1 secs or mins)
\end{enumerate}

\paragraph{Distributed Computing Challenges}
\begin{itemize}
\item \textbf{Scalability}: Independent parallel processing of
  sub-requests/tasks. More servers permits serving more concurrent requests.
\item \textbf{Fault tolerance}: Mask and recover from HW/SW failure. Replication
  data and service.
\item \textbf{High availability}
\item \textbf{Consistency}: $/neq$ services but $=$ results.
\item \textbf{Performance}: \textbf{Predictable} low-latency processing with
  high throughput
\end{itemize}

\paragraph{Abstractions}: Each layer should itself be scalable, network-aware and fault-tolerant.
\begin{figure}[h]
  \centering
    \includegraphics[width=0.59\textwidth]{img/abstraction.png}
\end{figure}

\paragraph{Design principles}
\begin{itemize}
\item \textbf{Stateless services}: Avoid data inconsistency. Separation of data +
  meta-data. If state needed, use leases to expire state gracefully.
\item \textbf{Caching}: Want low latency.
\item \textbf{Partition/aggregration pattern} (see google search above)
\item \textbf{Consistency models}: Need replication for reliability,  
  availability and low latency access. Most apps use a mix of strongly consistent and inconsistent operations. Weaker better for low latency but harder to reason about. Decide what matters (e.g. order of posts
  in LinkedIn news feed? Access from multiple devices?). 
\item \textbf{Efficient failure recovery}: Failure is very common, but full redundancy
  too expensive $\rightarrow$ use failure recovery (and try to reduce the cost of failure recovery)
  \begin{itemize}
    \item Replication: Need to replicate data and service (consistency issues)
      \item Recomputation: Use stateless protocols, form data lineage for compute jobs
  \end{itemize}
\end{itemize}